{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bfa023",
   "metadata": {},
   "source": [
    "#  <font color = darkblue size =6.5><center> Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c49161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5802db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\DELL\\Documents\\NLP\\Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065dc45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf01997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05d056a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97843043",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df.text,df.label,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4acee4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14307    I watched it last night and again this morning...\n",
       "17812    although i liked this Western,i do have to say...\n",
       "11020    I sat down to watch a documentary about Puerto...\n",
       "15158    This was probably intended as an \"arty\" crime ...\n",
       "24990    The summary provided by my cable TV guide made...\n",
       "                               ...                        \n",
       "6265     This movie is one of the worst movie i have ev...\n",
       "11284    This movie is inspiring to anyone who is or ha...\n",
       "38158    \"East Side Story\" is a documentary of musical ...\n",
       "860      And a self-admitted one to boot. At one point ...\n",
       "15795    This movie had horrible lighting and terrible ...\n",
       "Name: text, Length: 32000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c97e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14307    1\n",
       "17812    1\n",
       "11020    0\n",
       "15158    0\n",
       "24990    0\n",
       "        ..\n",
       "6265     0\n",
       "11284    1\n",
       "38158    1\n",
       "860      0\n",
       "15795    0\n",
       "Name: label, Length: 32000, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672f3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def tokenization(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0529debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_token = x_train.apply(tokenization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9646b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_token = x_test.apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bedc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14307    [I, watched, it, last, night, and, again, this...\n",
       "17812    [although, i, liked, this, Western, ,, i, do, ...\n",
       "11020    [I, sat, down, to, watch, a, documentary, abou...\n",
       "15158    [This, was, probably, intended, as, an, ``, ar...\n",
       "24990    [The, summary, provided, by, my, cable, TV, gu...\n",
       "                               ...                        \n",
       "6265     [This, movie, is, one, of, the, worst, movie, ...\n",
       "11284    [This, movie, is, inspiring, to, anyone, who, ...\n",
       "38158    [``, East, Side, Story, '', is, a, documentary...\n",
       "860      [And, a, self-admitted, one, to, boot, ., At, ...\n",
       "15795    [This, movie, had, horrible, lighting, and, te...\n",
       "Name: text, Length: 32000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5633301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    clean_text = [x for x in data if x.isalpha()]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93326fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_without_punt = x_train_token.apply(remove_punctuation)\n",
    "x_test_without_punt = x_test_token.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5aafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    lower = [x.lower() for x in data]\n",
    "    return lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "948e8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal = x_train_without_punt.apply(normalization)\n",
    "x_test_normal = x_test_without_punt.apply(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33ce989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_remove(data):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    clean_text = [x for x in data if x not in stop ]\n",
    "    return clean_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaa1c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1= x_train_normal.apply(stopwords_remove)\n",
    "x_test1= x_test_normal.apply(stopwords_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e72ae6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemma = WordNetLemmatizer()\n",
    "    l1 = []\n",
    "    for i in data :\n",
    "        text1 = lemma.lemmatize(i)\n",
    "        l1.append(text1)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a8717ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = x_train1.apply(lemmatization)\n",
    "final_test = x_test1.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8904379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14307    [watched, last, night, morning, much, liked, s...\n",
       "17812    [although, liked, western, say, one, favourite...\n",
       "11020    [sat, watch, documentary, puerto, rico, ended,...\n",
       "15158    [probably, intended, arty, crime, thriller, fa...\n",
       "24990    [summary, provided, cable, tv, guide, made, so...\n",
       "                               ...                        \n",
       "6265     [movie, one, worst, movie, ever, seen, life, w...\n",
       "11284    [movie, inspiring, anyone, tough, jam, whether...\n",
       "38158    [east, side, story, documentary, musical, come...\n",
       "860      [one, boot, one, point, doctor, assistant, ref...\n",
       "15795    [movie, horrible, lighting, terrible, camera, ...\n",
       "Name: text, Length: 32000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8233d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_list(data):\n",
    "    text = \" \".join(data)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0508ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_train = final_train.apply(join_list)\n",
    "final_text_test = final_test.apply(join_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbeda313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14307    watched last night morning much liked somethin...\n",
       "17812    although liked western say one favourite john ...\n",
       "11020    sat watch documentary puerto rico ended watchi...\n",
       "15158    probably intended arty crime thriller fails co...\n",
       "24990    summary provided cable tv guide made sound lot...\n",
       "                               ...                        \n",
       "6265     movie one worst movie ever seen life waste tim...\n",
       "11284    movie inspiring anyone tough jam whether finan...\n",
       "38158    east side story documentary musical comedy sta...\n",
       "860      one boot one point doctor assistant refers br ...\n",
       "15795    movie horrible lighting terrible camera moveme...\n",
       "Name: text, Length: 32000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199581b",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "243e1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d2290e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True,stop_words='english',max_df=0.95,max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1ef9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = cv.fit_transform(final_text_train)\n",
    "count_test = cv.transform(final_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eec81a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1425701 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34f9625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'fifteen', 'whose', 'how', 'fire', 'in', 'already', 'whom', 'could', 'myself', 'eg', 'once', 'whither', 'until', 'yourselves', 'should', 'moreover', 'off', 'either', 'between', 'yourself', 'whoever', 'below', 'via', 'own', 'or', 'only', 'elsewhere', 'hereafter', 'anywhere', 'nothing', 'would', 'we', 'co', 'toward', 'whether', 'show', 'had', 'front', 'us', 'for', 'too', 'always', 'within', 'thin', 'sixty', 'me', 'afterwards', 'as', 'get', 'due', 'now', 'amount', 'three', 'everyone', 'latter', 'another', 'such', 'thereupon', 'under', 'besides', 'be', 'if', 'becoming', 'therein', 'must', 'and', 'never', 'thick', 'has', 'may', 'con', 'nowhere', 'otherwise', 'first', 'per', 'after', 'give', 'therefore', 'more', 'which', 'twelve', 'twenty', 'been', 'also', 'see', 'same', 'wherein', 'name', 'interest', 'everywhere', 'anyone', 'someone', 'meanwhile', 'perhaps', 'himself', 'whereafter', 'towards', 'nine', 'against', 'can', 'my', 'two', 'ten', 'was', 'than', 'because', 'behind', 'i', 'any', 'formerly', 'before', 'into', 'upon', 'seem', 'still', 'all', 'a', 'most', 'you', 'thence', 'each', 'beforehand', 'seemed', 'not', 'next', 'without', 'further', 'nevertheless', 'others', 'somewhere', 'his', 'done', 'amongst', 'none', 'fill', 'seems', 'less', 'from', 'forty', 'hence', 'ever', 'else', 'find', 'whereupon', 'detail', 'are', 'somehow', 'its', 'their', 'both', 'mill', 'many', 'every', 'cant', 'empty', 'it', 'ltd', 'several', 'serious', 'beyond', 'enough', 'is', 'sincere', 'thru', 'bottom', 'wherever', 'whenever', 'cannot', 'almost', 'on', 'again', 'however', 'she', 'one', 'latterly', 'six', 'go', 'bill', 'last', 'mostly', 'during', 'put', 'with', 'mine', 'anyway', 'namely', 'un', 'hundred', 'whatever', 'etc', 'cry', 'become', 'but', 'about', 'themselves', 'through', 'were', 'yours', 'four', 'ourselves', 'your', 'above', 'over', 'top', 'couldnt', 'those', 'might', 'neither', 'side', 'throughout', 'who', 'anyhow', 'along', 'keep', 'no', 'these', 'de', 'here', 'him', 'whole', 'please', 'our', 'eleven', 'anything', 'indeed', 'though', 'while', 'herself', 'third', 'sometime', 'why', 'being', 'thus', 'her', 'five', 'often', 'well', 'where', 'he', 'inc', 'made', 'seeming', 'part', 'noone', 'amoungst', 'that', 'across', 'even', 'sometimes', 'this', 'an', 'although', 'becomes', 'former', 'take', 'whereby', 'back', 'full', 'describe', 'so', 'then', 'move', 'hers', 'have', 'thereby', 'least', 'fifty', 'hereupon', 'since', 'yet', 'whence', 'of', 'them', 'onto', 'when', 'alone', 'by', 'found', 'something', 'there', 'ie', 'together', 'became', 'up', 're', 'except', 'am', 'hasnt', 'to', 'itself', 'much', 'few', 'thereafter', 'call', 'around', 'beside', 'whereas', 'the', 'very', 'other', 'among', 'rather', 'they', 'some', 'system', 'what', 'at', 'nobody', 'will', 'eight', 'down', 'nor', 'everything', 'ours', 'hereby', 'do', 'out', 'herein'})\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0966b46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'absolutely', 'accent', 'act', 'acted', 'acting', 'action', 'actor', 'actress', 'actual', 'actually', 'adaptation', 'add', 'added', 'admit', 'adult', 'adventure', 'age', 'agent', 'ago', 'agree', 'air', 'alien', 'alive', 'amazing', 'america', 'american', 'amusing', 'animal', 'animated', 'animation', 'annoying', 'answer', 'apart', 'apparently', 'appeal', 'appear', 'appearance', 'appears', 'appreciate', 'area', 'army', 'art', 'artist', 'aside', 'ask', 'aspect', 'atmosphere', 'attack', 'attempt', 'attention', 'audience', 'average', 'avoid', 'award', 'away', 'awesome', 'awful', 'baby', 'background', 'bad', 'badly', 'band', 'barely', 'based', 'basic', 'basically', 'battle', 'beat', 'beautiful', 'beautifully', 'beauty', 'bed', 'begin', 'beginning', 'belief', 'believable', 'believe', 'ben', 'best', 'better', 'big', 'biggest', 'bit', 'bizarre', 'black', 'blood', 'blue', 'body', 'bond', 'book', 'bored', 'boring', 'bother', 'bought', 'box', 'boy', 'br', 'brain', 'break', 'brilliant', 'bring', 'brings', 'british', 'brother', 'brought', 'bruce', 'budget', 'building', 'bunch', 'business', 'buy', 'ca', 'called', 'came', 'camera', 'camp', 'capture', 'car', 'care', 'career', 'carry', 'cartoon', 'case', 'cast', 'casting', 'cat', 'catch', 'caught', 'cause', 'century', 'certain', 'certainly', 'chance', 'change', 'changed', 'channel', 'character', 'charles', 'charlie', 'charm', 'charming', 'chase', 'cheap', 'check', 'cheesy', 'chemistry', 'child', 'choice', 'christmas', 'cinema', 'cinematic', 'cinematography', 'city', 'class', 'classic', 'clear', 'clearly', 'clever', 'close', 'club', 'cold', 'college', 'color', 'come', 'comedy', 'comic', 'coming', 'comment', 'common', 'company', 'compared', 'complete', 'completely', 'complex', 'computer', 'concept', 'conclusion', 'consider', 'considered', 'considering', 'control', 'convincing', 'cool', 'cop', 'copy', 'cost', 'costume', 'country', 'couple', 'course', 'cover', 'crap', 'crazy', 'create', 'created', 'creature', 'credit', 'creepy', 'crew', 'crime', 'criminal', 'critic', 'cult', 'culture', 'cut', 'cute', 'dad', 'dance', 'dancing', 'dark', 'date', 'daughter', 'david', 'day', 'dead', 'deal', 'death', 'decent', 'decide', 'decided', 'decides', 'deep', 'definitely', 'depth', 'deserves', 'despite', 'detective', 'development', 'dialog', 'dialogue', 'die', 'died', 'difference', 'different', 'difficult', 'directed', 'directing', 'direction', 'director', 'disappointed', 'disney', 'disturbing', 'doctor', 'documentary', 'dog', 'door', 'doubt', 'drama', 'dramatic', 'dream', 'drive', 'drug', 'dull', 'dumb', 'dvd', 'earlier', 'early', 'earth', 'easily', 'easy', 'edge', 'editing', 'effect', 'effective', 'effort', 'element', 'emotion', 'emotional', 'end', 'ended', 'ending', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'entertaining', 'entertainment', 'entire', 'entirely', 'episode', 'era', 'escape', 'especially', 'event', 'eventually', 'everybody', 'evil', 'exactly', 'example', 'excellent', 'exception', 'exciting', 'excuse', 'expect', 'expectation', 'expected', 'expecting', 'experience', 'explain', 'extra', 'extremely', 'eye', 'face', 'fact', 'failed', 'fails', 'fair', 'fairly', 'fake', 'fall', 'familiar', 'family', 'famous', 'fan', 'fantastic', 'fantasy', 'far', 'fast', 'father', 'favorite', 'fear', 'feature', 'feel', 'feeling', 'felt', 'female', 'festival', 'fiction', 'fight', 'fighting', 'figure', 'filled', 'film', 'filmed', 'filmmaker', 'final', 'finally', 'fine', 'finish', 'fit', 'flat', 'flaw', 'flick', 'focus', 'folk', 'follow', 'following', 'follows', 'foot', 'footage', 'force', 'forced', 'forget', 'form', 'forward', 'frank', 'free', 'french', 'friend', 'fun', 'funny', 'future', 'gag', 'game', 'gang', 'garbage', 'gave', 'gay', 'gem', 'general', 'generally', 'genius', 'genre', 'george', 'german', 'getting', 'ghost', 'giant', 'girl', 'girlfriend', 'given', 'giving', 'glad', 'god', 'going', 'gone', 'good', 'gore', 'got', 'government', 'great', 'greatest', 'group', 'guess', 'gun', 'guy', 'hair', 'half', 'hand', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardly', 'hate', 'head', 'hear', 'heard', 'heart', 'hell', 'help', 'hero', 'high', 'highly', 'hilarious', 'history', 'hit', 'hold', 'hole', 'hollywood', 'home', 'honest', 'honestly', 'hope', 'horrible', 'horror', 'horse', 'hot', 'hour', 'house', 'huge', 'human', 'humor', 'humour', 'hurt', 'husband', 'idea', 'image', 'imagine', 'imdb', 'immediately', 'important', 'impossible', 'impression', 'impressive', 'including', 'incredible', 'incredibly', 'indian', 'innocent', 'inside', 'instead', 'intelligent', 'interested', 'interesting', 'involved', 'involving', 'island', 'issue', 'italian', 'jack', 'james', 'jane', 'japanese', 'jim', 'job', 'joe', 'john', 'joke', 'jones', 'journey', 'kept', 'kid', 'kill', 'killed', 'killer', 'killing', 'kind', 'king', 'knew', 'know', 'known', 'la', 'lack', 'lady', 'lame', 'land', 'language', 'large', 'late', 'later', 'laugh', 'laughing', 'law', 'le', 'lead', 'leading', 'leaf', 'learn', 'leave', 'leaving', 'lee', 'left', 'let', 'level', 'lie', 'life', 'light', 'like', 'liked', 'likely', 'line', 'list', 'literally', 'little', 'live', 'living', 'local', 'location', 'long', 'longer', 'look', 'looked', 'looking', 'lost', 'lot', 'love', 'loved', 'lover', 'low', 'mad', 'magic', 'main', 'major', 'make', 'maker', 'making', 'male', 'man', 'managed', 'manages', 'mark', 'married', 'mary', 'master', 'masterpiece', 'match', 'material', 'matter', 'maybe', 'mean', 'meaning', 'meant', 'meet', 'member', 'memorable', 'memory', 'men', 'mention', 'mentioned', 'mess', 'message', 'michael', 'middle', 'million', 'mind', 'minor', 'minute', 'miss', 'missed', 'missing', 'mistake', 'modern', 'moment', 'money', 'monster', 'mood', 'moral', 'mother', 'movie', 'moving', 'mr', 'murder', 'music', 'musical', 'mystery', 'named', 'nature', 'near', 'nearly', 'need', 'needed', 'new', 'nice', 'night', 'normal', 'note', 'novel', 'nudity', 'number', 'obvious', 'obviously', 'odd', 'offer', 'office', 'oh', 'ok', 'okay', 'old', 'older', 'open', 'opening', 'opera', 'opinion', 'opportunity', 'order', 'original', 'oscar', 'outside', 'overall', 'pace', 'parent', 'park', 'particular', 'particularly', 'party', 'past', 'pathetic', 'paul', 'pay', 'people', 'perfect', 'perfectly', 'performance', 'period', 'person', 'personal', 'personality', 'personally', 'peter', 'photography', 'pick', 'picture', 'piece', 'place', 'plain', 'plan', 'plane', 'planet', 'play', 'played', 'player', 'playing', 'plenty', 'plot', 'plus', 'point', 'pointless', 'police', 'political', 'poor', 'poorly', 'popular', 'portrayal', 'portrayed', 'positive', 'possible', 'possibly', 'potential', 'power', 'powerful', 'predictable', 'premise', 'present', 'pretty', 'previous', 'prison', 'probably', 'problem', 'produced', 'producer', 'production', 'project', 'public', 'pull', 'pure', 'purpose', 'quality', 'question', 'quickly', 'quite', 'race', 'rate', 'rating', 'ray', 'read', 'reading', 'real', 'realistic', 'reality', 'realize', 'really', 'reason', 'recent', 'recently', 'recommend', 'recommended', 'red', 'reference', 'relationship', 'release', 'released', 'remains', 'remake', 'remember', 'rent', 'respect', 'rest', 'result', 'return', 'revenge', 'review', 'reviewer', 'rich', 'richard', 'ride', 'ridiculous', 'right', 'ring', 'road', 'robert', 'rock', 'role', 'romance', 'romantic', 'room', 'run', 'running', 'sad', 'sadly', 'said', 'sam', 'save', 'saw', 'say', 'saying', 'scary', 'scene', 'scenery', 'school', 'science', 'scientist', 'score', 'scott', 'screen', 'screenplay', 'script', 'season', 'second', 'secret', 'seeing', 'seen', 'sense', 'sequel', 'sequence', 'series', 'seriously', 'set', 'setting', 'sex', 'sexual', 'sexy', 'shame', 'ship', 'shoot', 'shooting', 'short', 'shot', 'showed', 'showing', 'shown', 'sick', 'silent', 'silly', 'similar', 'simple', 'simply', 'singing', 'single', 'sister', 'sit', 'sitting', 'situation', 'slasher', 'slightly', 'slow', 'small', 'social', 'society', 'soldier', 'solid', 'somewhat', 'son', 'song', 'soon', 'sorry', 'sort', 'soul', 'sound', 'soundtrack', 'south', 'space', 'speak', 'special', 'spend', 'spent', 'spirit', 'spoiler', 'spot', 'stage', 'stand', 'standard', 'star', 'starring', 'start', 'started', 'state', 'stay', 'steal', 'step', 'steve', 'stick', 'stop', 'store', 'story', 'storyline', 'straight', 'strange', 'street', 'strong', 'struggle', 'student', 'studio', 'stuff', 'stupid', 'style', 'subject', 'subtle', 'success', 'successful', 'suck', 'suddenly', 'superb', 'supporting', 'supposed', 'sure', 'surprise', 'surprised', 'surprisingly', 'suspect', 'suspense', 'sweet', 'taken', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking', 'taste', 'team', 'teen', 'teenager', 'television', 'tell', 'telling', 'tension', 'term', 'terrible', 'thanks', 'theater', 'theme', 'thing', 'think', 'thinking', 'thought', 'thriller', 'throw', 'time', 'title', 'today', 'told', 'tom', 'tone', 'took', 'total', 'totally', 'touch', 'tough', 'town', 'track', 'trailer', 'train', 'trash', 'tried', 'trip', 'trouble', 'true', 'truly', 'truth', 'try', 'trying', 'turn', 'turned', 'tv', 'twist', 'type', 'typical', 'ultimately', 'understand', 'unfortunately', 'unique', 'unless', 'unlike', 'use', 'used', 'using', 'usual', 'usually', 'utterly', 'value', 'vampire', 'van', 'various', 'version', 'victim', 'video', 'view', 'viewer', 'viewing', 'villain', 'violence', 'violent', 'visual', 'voice', 'wait', 'waiting', 'walk', 'walking', 'wall', 'want', 'wanted', 'war', 'waste', 'wasted', 'watch', 'watched', 'watching', 'water', 'way', 'weak', 'week', 'weird', 'went', 'west', 'western', 'white', 'wife', 'william', 'win', 'wish', 'wo', 'woman', 'wonder', 'wonderful', 'wood', 'word', 'work', 'worked', 'working', 'world', 'worse', 'worst', 'worth', 'write', 'writer', 'writing', 'written', 'wrong', 'wrote', 'yeah', 'year', 'yes', 'york', 'young', 'younger', 'zombie']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2832663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88a94f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(count_train.A,columns = cv.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b294c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68c20d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(count_train.A,y_train)\n",
    "pred_mnb = mnb.predict(count_test.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b4d45e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "275eb92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.823625"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(y_test,pred_mnb)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214ab1c",
   "metadata": {},
   "source": [
    "#  TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7eb5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f887c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(lowercase=True,stop_words='english',max_df=0.95,max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "187d2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf.fit_transform(final_text_train)\n",
    "tf_test = tf.transform(final_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d700ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accent</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183988</td>\n",
       "      <td>0.243297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  absolutely  accent  act  acted    acting    action  actor  \\\n",
       "0      0.0   0.0         0.0     0.0  0.0    0.0  0.000000  0.000000    0.0   \n",
       "1      0.0   0.0         0.0     0.0  0.0    0.0  0.000000  0.167756    0.0   \n",
       "2      0.0   0.0         0.0     0.0  0.0    0.0  0.000000  0.000000    0.0   \n",
       "3      0.0   0.0         0.0     0.0  0.0    0.0  0.183988  0.243297    0.0   \n",
       "4      0.0   0.0         0.0     0.0  0.0    0.0  0.076361  0.000000    0.0   \n",
       "\n",
       "   actress  ...  written  wrong  wrote  yeah  year  yes  york  young  younger  \\\n",
       "0      0.0  ...      0.0    0.0    0.0   0.0   0.0  0.0   0.0    0.0      0.0   \n",
       "1      0.0  ...      0.0    0.0    0.0   0.0   0.0  0.0   0.0    0.0      0.0   \n",
       "2      0.0  ...      0.0    0.0    0.0   0.0   0.0  0.0   0.0    0.0      0.0   \n",
       "3      0.0  ...      0.0    0.0    0.0   0.0   0.0  0.0   0.0    0.0      0.0   \n",
       "4      0.0  ...      0.0    0.0    0.0   0.0   0.0  0.0   0.0    0.0      0.0   \n",
       "\n",
       "   zombie  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_train.A,columns = tf.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027787a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0e4e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1422b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(tf_train.A,y_train)\n",
    "pred_mnb = mnb.predict(tf_test.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a799bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2847b284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82825"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(y_test,pred_mnb)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c5052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
